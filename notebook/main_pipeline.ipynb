{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPlb+cPjqncXRVkgT2OVW4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luquelab/bioinformatics-teamCanes/blob/main/notebook/main_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Sequence Analysis and Functional Prediction Pipeline Notebook\n",
        "\n",
        "This notebook performs a thorough analysis of a group of nucleotide sequences. The pipeline includes:\n",
        "- **Sequence Analysis**: Uploading FASTA sequences, computing sequence lengths, GC content, and performing translations.\n",
        "- **Visualization**: Plotting sequence length and GC content, generating similarity matrices with heatmaps.\n",
        "- **Alignments & Phylogenetic Trees**: Pairwise alignments, computing percent identities, and creating phylogenetic trees.\n",
        "- **Functional & Organism Prediction**: Running online BLAST and HMMER searches to predict potential functions and organismal origins.\n",
        "- **Directory Structure**: The code automatically creates a directory structure to keep data, results, and intermediate files organized.\n",
        "\n",
        "This notebook is intended to be run in a Google Colab environment because it utilizes the `google.colab` module for file uploads and drive access.\n",
        "\n",
        "To test this pipeline, use the \"sequences.fasta\" test file.\n"
      ],
      "metadata": {
        "id": "FCpM2Gi6brha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCoxPDRMZ7mC"
      },
      "outputs": [],
      "source": [
        "!pip install biopython lxml\n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from Bio import SeqIO, Phylo\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import pairwise2\n",
        "from Bio.Phylo.TreeConstruction import DistanceMatrix, DistanceTreeConstructor\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from lxml import etree\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Directory Structure Setup\n",
        "\n",
        "This section defines and creates a directory structure to organize your project files. The structure includes folders for data, results (with several sub-folders for different analyses), and binaries.\n",
        "\n",
        "Modify your root_dir variable to match desired directory name."
      ],
      "metadata": {
        "id": "1eR0T-MKeUDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the assignment root name as needed (e.g., project_x_analysis)\n",
        "root_dir = \"project_x_analysis\"\n",
        "dirs = {\n",
        "    \"data\": os.path.join(root_dir, \"data\"),\n",
        "    \"bin\": os.path.join(root_dir, \"bin\"),\n",
        "    \"sequence_properties\": os.path.join(root_dir, \"results\", \"sequence_properties\"),\n",
        "    \"alignments\": os.path.join(root_dir, \"results\", \"alignments\"),\n",
        "    \"phylogenetic_tree\": os.path.join(root_dir, \"results\", \"phylogenetic_tree\"),\n",
        "    \"functional_prediction\": os.path.join(root_dir, \"results\", \"functional_prediction\"),\n",
        "    \"organism_origin\": os.path.join(root_dir, \"results\", \"organism_origin\"),\n",
        "}\n",
        "\n",
        "for d in dirs.values():\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Define the assignment bin directory\n",
        "bin_dir = os.path.join(root_dir, \"bin\")\n",
        "os.makedirs(bin_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "mJWqvOfDeQdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Sequence Analysis Pipeline\n",
        "\n",
        "This portion of the notebook performs several tasks:\n",
        "1. **Sequence Upload**: Prompts the user to upload a FASTA file.\n",
        "2. **Sequence Properties Calculation**: Reads the FASTA file, computes basic properties such as the sequence length (in bp and kb) and GC content, and translates the nucleotide sequences into protein sequences.\n",
        "3. **Visualizations**: Plots sequence length and GC content bar charts and saves the figures.\n",
        "4. **Pairwise Alignments and Similarity Calculations**: Uses pairwise global alignments to compute percent identity between sequences (for both nucleotide and translated protein sequences) and outputs the results.\n",
        "5. **Similarity Matrices and Heatmaps**: Constructs full similarity matrices and visualizes them as heatmaps.\n",
        "6. **Phylogenetic Tree Construction**: Generates distance matrices and constructs neighbor-joining (NJ) trees for both nucleotide and protein sequences."
      ],
      "metadata": {
        "id": "a3RPegSPe5aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Upload and Store Sequences File\n",
        "\n",
        "Use the interactive file upload widget provided by Google Colab to upload your FASTA file. The file is then copied into the `data` folder.\n",
        "\n",
        "An example .fasta is provided in the repo as \"sequences.fna\"."
      ],
      "metadata": {
        "id": "kBcgj7ezfBaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload your FASTA file (e.g., sequences.fna) ...\")\n",
        "uploaded = files.upload()  # Interactive file upload for FASTA file\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {filename}\")\n",
        "\n",
        "# Move the uploaded file to the data/ folder.\n",
        "data_file = os.path.join(dirs[\"data\"], \"sequences.fasta\")\n",
        "shutil.copy(filename, data_file)\n",
        "print(f\"Copied file to: {data_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "cHXH_pUje3bz",
        "outputId": "34099148-0427-4d28-fb01-eaab59315b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your FASTA file (e.g., sequences.fna) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c82004c7-73b0-4871-abc7-f5c4a602c8c5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c82004c7-73b0-4871-abc7-f5c4a602c8c5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sequences.fasta to sequences (1).fasta\n",
            "Uploaded file: sequences (1).fasta\n",
            "Copied file to: project_x_analysis/data/sequences.fasta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load Sequences and Compute Basic Properties\n",
        "\n",
        "This cell:\n",
        "- Loads the sequences from the uploaded FASTA file.\n",
        "- Calculates the length (in bp and kb) and GC content for each sequence.\n",
        "- Translates each nucleotide sequence into a protein sequence (translation halts at the first stop codon).\n",
        "- Saves these computed properties into a CSV file.\n",
        "- Generates bar charts for sequence lengths and GC content."
      ],
      "metadata": {
        "id": "bHz3FA2GfKOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "records = list(SeqIO.parse(data_file, \"fasta\"))\n",
        "print(f\"You successfully loaded {len(records)} sequences.\")\n",
        "\n",
        "def compute_gc(seq):\n",
        "    \"\"\"Compute GC content percentage.\"\"\"\n",
        "    seq = seq.upper()\n",
        "    return 100 * (seq.count(\"G\") + seq.count(\"C\")) / len(seq) if len(seq) > 0 else 0\n",
        "\n",
        "data_props = []\n",
        "protein_records = []\n",
        "for record in records:\n",
        "    seq_str = str(record.seq)\n",
        "    length_bp = len(seq_str)\n",
        "    gc_content = compute_gc(seq_str)\n",
        "    # Translate (stop at the first stop codon)\n",
        "    protein_seq = record.seq.translate(to_stop=True)\n",
        "    protein_records.append(SeqRecord(protein_seq, id=record.id, description=\"Translated sequence\"))\n",
        "    data_props.append({\n",
        "        \"Sequence_ID\": record.id,\n",
        "        \"Length_bp\": length_bp,\n",
        "        \"Length_kb\": length_bp / 1000,\n",
        "        \"GC_Content (%)\": gc_content\n",
        "    })\n",
        "\n",
        "df_props = pd.DataFrame(data_props)\n",
        "print(df_props)\n",
        "\n",
        "# Save sequence properties.\n",
        "props_csv = os.path.join(dirs[\"sequence_properties\"], \"properties.csv\")\n",
        "df_props.to_csv(props_csv, index=False)\n",
        "print(f\"Sequence properties saved to: {props_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddjLBQMnfK4r",
        "outputId": "002f27c5-d49a-4687-b908-b5b5218bbd34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You successfully loaded 5 sequences.\n",
            "  Sequence_ID  Length_bp  Length_kb  GC_Content (%)\n",
            "0  EF990648.1       1186      1.186       42.327150\n",
            "1  GU220721.1       1194      1.194       48.576214\n",
            "2  JX429970.1       1216      1.216       47.286184\n",
            "3  KU291912.1       1310      1.310       48.778626\n",
            "4  JN107639.1       1350      1.350       48.148148\n",
            "Sequence properties saved to: project_x_analysis/results/sequence_properties/properties.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/Seq.py:2879: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing Sequence Properties\n",
        "\n",
        "The following code generates bar charts for:\n",
        "- **Sequence Lengths (in kb)**\n",
        "- **GC Content (%)**\n",
        "\n",
        "Each figure is saved as a high-resolution PNG in the `sequence_properties` folder."
      ],
      "metadata": {
        "id": "7dqZQEAsfUBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save sequence lengths.\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(df_props[\"Sequence_ID\"], df_props[\"Length_kb\"])\n",
        "plt.xlabel(\"Sequence ID\")\n",
        "plt.ylabel(\"Length (kb)\")\n",
        "plt.title(\"Sequence Lengths\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "lengths_png = os.path.join(dirs[\"sequence_properties\"], \"lengths.png\")\n",
        "plt.savefig(lengths_png, dpi=300)\n",
        "plt.show()\n",
        "print(f\"Length plot saved to: {lengths_png}\")\n",
        "\n",
        "# Plot and save GC content.\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(df_props[\"Sequence_ID\"], df_props[\"GC_Content (%)\"])\n",
        "plt.xlabel(\"Sequence ID\")\n",
        "plt.ylabel(\"GC Content (%)\")\n",
        "plt.title(\"GC Content of Sequences\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "gc_png = os.path.join(dirs[\"sequence_properties\"], \"gc_content.png\")\n",
        "plt.savefig(gc_png, dpi=300)\n",
        "plt.show()\n",
        "print(f\"GC content plot saved to: {gc_png}\")\n",
        "\n",
        "# Save translated protein sequences to FASTA.\n",
        "translated_faa = os.path.join(dirs[\"sequence_properties\"], \"sequences_translated.faa\")\n",
        "SeqIO.write(protein_records, translated_faa, \"fasta\")\n",
        "print(f\"Translated protein sequences saved to: {translated_faa}\")"
      ],
      "metadata": {
        "id": "ZaVa5_LxfVDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Pairwise Alignments & Similarity Calculations\n",
        "\n",
        "This section calculates the percent identity between pairs of sequences:\n",
        "- **Pairwise Alignments**: Global alignments for both nucleotide and protein sequences are computed.\n",
        "- **Percent Identity**: Functions are provided to compute the percent identity from the alignments.\n",
        "- The results are saved in a CSV file along with the formatted alignments."
      ],
      "metadata": {
        "id": "LTCpjB0AfY7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_percent_identity(aln_seq1, aln_seq2):\n",
        "    matches = sum(a == b for a, b in zip(aln_seq1, aln_seq2) if a != '-' and b != '-')\n",
        "    alignment_length = max(len(aln_seq1), len(aln_seq2))\n",
        "    return (matches / alignment_length) * 100\n",
        "\n",
        "def compute_percent_identity(seq1, seq2):\n",
        "    aln = pairwise2.align.globalxx(seq1, seq2, one_alignment_only=True)[0]\n",
        "    return calc_percent_identity(aln.seqA, aln.seqB)\n",
        "\n",
        "similarity_data = []\n",
        "nuc_alignment_file = os.path.join(dirs[\"alignments\"], \"sequence_alignments_nucleotide.fna\")\n",
        "prot_alignment_file = os.path.join(dirs[\"alignments\"], \"sequence_alignments_proteins.faa\")\n",
        "\n",
        "with open(nuc_alignment_file, \"w\") as nuc_out, open(prot_alignment_file, \"w\") as prot_out:\n",
        "    for i in range(len(records)):\n",
        "        for j in range(i + 1, len(records)):\n",
        "            rec1, rec2 = records[i], records[j]\n",
        "            # Nucleotide alignment\n",
        "            aln_nuc = pairwise2.align.globalxx(rec1.seq, rec2.seq, one_alignment_only=True)[0]\n",
        "            nuc_out.write(f\"Alignment between {rec1.id} and {rec2.id}:\\n\")\n",
        "            nuc_out.write(pairwise2.format_alignment(*aln_nuc))\n",
        "            nuc_out.write(\"\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
        "            nuc_id = calc_percent_identity(aln_nuc.seqA, aln_nuc.seqB)\n",
        "            # Protein alignment\n",
        "            prot_aln = pairwise2.align.globalxx(protein_records[i].seq, protein_records[j].seq, one_alignment_only=True)[0]\n",
        "            prot_out.write(f\"Alignment between {rec1.id} and {rec2.id}:\\n\")\n",
        "            prot_out.write(pairwise2.format_alignment(*prot_aln))\n",
        "            prot_out.write(\"\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
        "            prot_id = calc_percent_identity(prot_aln.seqA, prot_aln.seqB)\n",
        "            similarity_data.append({\n",
        "                \"Sequence1\": rec1.id,\n",
        "                \"Sequence2\": rec2.id,\n",
        "                \"Nucleotide_percent_identity\": round(nuc_id, 2),\n",
        "                \"Protein_percent_identity\": round(prot_id, 2)\n",
        "            })\n",
        "\n",
        "similarity_csv = os.path.join(dirs[\"alignments\"], \"sequence_similarities.csv\")\n",
        "df_similarity = pd.DataFrame(similarity_data)\n",
        "df_similarity.to_csv(similarity_csv, index=False)\n",
        "print(f\"Pairwise similarity CSV saved to: {similarity_csv}\")"
      ],
      "metadata": {
        "id": "1c0J50avfdpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Full Similarity Matrices and Heatmaps\n",
        "\n",
        "This section generates full similarity matrices for nucleotide and protein sequences by computing pairwise percent identities. The matrices are saved as CSV files and visualized as heatmaps."
      ],
      "metadata": {
        "id": "qo2fV8-ofkKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_nuc = len(records)\n",
        "nuc_sim_matrix = np.zeros((num_nuc, num_nuc))\n",
        "for i in range(num_nuc):\n",
        "    for j in range(num_nuc):\n",
        "        nuc_sim_matrix[i, j] = compute_percent_identity(str(records[i].seq), str(records[j].seq))\n",
        "df_nuc_sim = pd.DataFrame(nuc_sim_matrix, index=[r.id for r in records], columns=[r.id for r in records])\n",
        "nuc_matrix_csv = os.path.join(dirs[\"alignments\"], \"nucleotide_similarity_matrix.csv\")\n",
        "df_nuc_sim.to_csv(nuc_matrix_csv, index=True)\n",
        "print(f\"Nucleotide similarity matrix saved to: {nuc_matrix_csv}\")\n",
        "\n",
        "num_prot = len(protein_records)\n",
        "prot_sim_matrix = np.zeros((num_prot, num_prot))\n",
        "for i in range(num_prot):\n",
        "    for j in range(num_prot):\n",
        "        prot_sim_matrix[i, j] = compute_percent_identity(str(protein_records[i].seq), str(protein_records[j].seq))\n",
        "df_prot_sim = pd.DataFrame(prot_sim_matrix, index=[r.id for r in protein_records], columns=[r.id for r in protein_records])\n",
        "prot_matrix_csv = os.path.join(dirs[\"alignments\"], \"protein_similarity_matrix.csv\")\n",
        "df_prot_sim.to_csv(prot_matrix_csv, index=True)\n",
        "print(f\"Protein similarity matrix saved to: {prot_matrix_csv}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(df_nuc_sim, interpolation='nearest', cmap='viridis')\n",
        "plt.colorbar(label='Percent Identity (%)')\n",
        "plt.xticks(range(num_nuc), df_nuc_sim.columns, rotation=45)\n",
        "plt.yticks(range(num_nuc), df_nuc_sim.index)\n",
        "plt.title(\"Nucleotide Similarity Matrix\")\n",
        "plt.tight_layout()\n",
        "nuc_heatmap_png = os.path.join(dirs[\"alignments\"], \"nucleotide_similarity_heatmap.png\")\n",
        "plt.savefig(nuc_heatmap_png, dpi=300)\n",
        "plt.show()\n",
        "print(f\"Nucleotide similarity heatmap saved to: {nuc_heatmap_png}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(df_prot_sim, interpolation='nearest', cmap='viridis')\n",
        "plt.colorbar(label='Percent Identity (%)')\n",
        "plt.xticks(range(num_prot), df_prot_sim.columns, rotation=45)\n",
        "plt.yticks(range(num_prot), df_prot_sim.index)\n",
        "plt.title(\"Protein Similarity Matrix\")\n",
        "plt.tight_layout()\n",
        "prot_heatmap_png = os.path.join(dirs[\"alignments\"], \"protein_similarity_heatmap.png\")\n",
        "plt.savefig(prot_heatmap_png, dpi=300)\n",
        "plt.show()\n",
        "print(f\"Protein similarity heatmap saved to: {prot_heatmap_png}\")"
      ],
      "metadata": {
        "id": "Pb6-H712flsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Build Phylogenetic Trees\n",
        "\n",
        "Using the computed similarity matrices, this section:\n",
        "- Converts the similarity data into distance matrices.\n",
        "- Constructs phylogenetic trees (neighbor-joining trees) for both nucleotide and protein sequences.\n",
        "- Saves the trees in Newick format and also saves visualizations of the trees."
      ],
      "metadata": {
        "id": "f_hoFcTufqyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_distance_matrix_with_diag(df_sim):\n",
        "    names = list(df_sim.index)\n",
        "    num_seqs = len(names)\n",
        "    lower_triangle = []\n",
        "    for i in range(num_seqs):\n",
        "        row = []\n",
        "        for j in range(i):\n",
        "            distance = 1 - (float(df_sim.iat[i, j]) / 100.0)\n",
        "            row.append(distance)\n",
        "        row.append(0.0)\n",
        "        lower_triangle.append(row)\n",
        "    return DistanceMatrix(names, lower_triangle)\n",
        "\n",
        "nuc_dm = build_distance_matrix_with_diag(df_nuc_sim)\n",
        "constructor = DistanceTreeConstructor()\n",
        "nuc_tree = constructor.nj(nuc_dm)\n",
        "nuc_tree_file = os.path.join(dirs[\"phylogenetic_tree\"], \"tree_nucleotides.nwk\")\n",
        "Phylo.write(nuc_tree, nuc_tree_file, \"newick\")\n",
        "print(f\"Nucleotide tree (Newick) saved to: {nuc_tree_file}\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "Phylo.draw(nuc_tree, do_show=False)\n",
        "nuc_tree_png = os.path.join(dirs[\"phylogenetic_tree\"], \"tree_nucleotides_visualization.png\")\n",
        "plt.savefig(nuc_tree_png, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"Nucleotide tree visualization saved to: {nuc_tree_png}\")\n",
        "\n",
        "prot_dm = build_distance_matrix_with_diag(df_prot_sim)\n",
        "prot_tree = constructor.nj(prot_dm)\n",
        "prot_tree_file = os.path.join(dirs[\"phylogenetic_tree\"], \"tree_proteins.nwk\")\n",
        "Phylo.write(prot_tree, prot_tree_file, \"newick\")\n",
        "print(f\"Protein tree (Newick) saved to: {prot_tree_file}\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "Phylo.draw(prot_tree, do_show=False)\n",
        "prot_tree_png = os.path.join(dirs[\"phylogenetic_tree\"], \"tree_proteins_visualization.png\")\n",
        "plt.savefig(prot_tree_png, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"Protein tree visualization saved to: {prot_tree_png}\")\n",
        "\n",
        "print(\"\\n=== Sequence Analysis Pipeline Completed ===\")"
      ],
      "metadata": {
        "id": "ImYK2uhTftn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Functional & Organism Prediction Pipeline\n",
        "\n",
        "This section leverages online tools to predict the function of your sequences and their organismal origin. The workflow includes:\n",
        "- **BLAST Search**: Each sequence is used to perform a BLAST search against the NCBI database. The resulting XML files are parsed to extract hit definitions and predicted organism information.\n",
        "- **HMMER Search**: An online HMMER search (against the Pfam database) is conducted using the translated protein sequence to predict domain structures.\n",
        "- The predictions are then aggregated and saved into CSV files.\n",
        "- Finally, corresponding functional prediction files are copied to organism-specific folders for further analysis."
      ],
      "metadata": {
        "id": "TvU4XfTDf2rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blast_program = \"blastx\"\n",
        "blast_db = \"nr\"\n",
        "blast_hitlist_size = 5\n",
        "use_online_hmmer = True   # Set to False if using local hmmscan\n",
        "hmmdb_online = \"pfam\"\n",
        "\n",
        "domain_predictions = []\n",
        "organism_predictions = []\n",
        "organism_overall = []\n",
        "\n",
        "for record in records:\n",
        "    seq_id = record.id\n",
        "    # Create separate folders per sequence under functional_prediction and organism_origin\n",
        "    seq_func_folder = os.path.join(dirs[\"functional_prediction\"], seq_id)\n",
        "    seq_org_folder  = os.path.join(dirs[\"organism_origin\"], seq_id)\n",
        "    os.makedirs(seq_func_folder, exist_ok=True)\n",
        "    os.makedirs(seq_org_folder, exist_ok=True)\n",
        "\n",
        "    # --- BLAST Search ---\n",
        "    print(f\"Running BLAST for {seq_id} ...\")\n",
        "    try:\n",
        "        blast_handle = NCBIWWW.qblast(blast_program, blast_db, record.seq, hitlist_size=blast_hitlist_size)\n",
        "        blast_xml = blast_handle.read()\n",
        "        blast_handle.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during BLAST search for {seq_id}: {e}\")\n",
        "        blast_xml = \"\"\n",
        "    blast_outfile = os.path.join(seq_func_folder, f\"{seq_id}_blast.xml\")\n",
        "    with open(blast_outfile, \"w\") as f:\n",
        "        f.write(blast_xml)\n",
        "    print(f\"BLAST results saved to {blast_outfile}\")\n",
        "\n",
        "    function_preds = []\n",
        "    organism_preds = []\n",
        "    try:\n",
        "        with open(blast_outfile) as result_handle:\n",
        "            blast_record = NCBIXML.read(result_handle)\n",
        "            if blast_record.alignments:\n",
        "                for hit in blast_record.alignments[:blast_hitlist_size]:\n",
        "                    function_preds.append(hit.hit_def)\n",
        "                    org_match = re.search(r'\\[(.*?)\\]', hit.hit_def)\n",
        "                    if org_match:\n",
        "                        organism_preds.append(org_match.group(1))\n",
        "                    else:\n",
        "                        organism_preds.append(\"Unknown\")\n",
        "            else:\n",
        "                print(f\"No BLAST hits found for {seq_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing BLAST result for {seq_id}: {e}\")\n",
        "\n",
        "    function_pred = \"; \".join(function_preds) if function_preds else \"No hit\"\n",
        "    organism_pred = \"; \".join(organism_preds) if organism_preds else \"Unknown\"\n",
        "    organism_predictions.append([seq_id, organism_pred, function_pred])\n",
        "\n",
        "    # --- HMMER Search ---\n",
        "    protein_seq = record.seq.translate(to_stop=True)\n",
        "    if use_online_hmmer:\n",
        "        def run_online_hmmer(prot_seq, seq_id):\n",
        "            submit_url = \"https://www.ebi.ac.uk/Tools/hmmer/search/hmmscan\"\n",
        "            data = {\"hmmdb\": hmmdb_online, \"seq\": str(prot_seq), \"domE\": \"1e-5\"}\n",
        "            headers = {\"Expect\": \"\", \"Accept\": \"text/xml\"}\n",
        "            print(f\"Submitting online HMMER job for {seq_id} ...\")\n",
        "            response = requests.post(submit_url, data=data, headers=headers)\n",
        "            if response.status_code != 200:\n",
        "                raise Exception(f\"HMMER error for {seq_id}: {response.status_code}\")\n",
        "            return response.text\n",
        "        try:\n",
        "            hmm_xml = run_online_hmmer(protein_seq, seq_id)\n",
        "            hmm_outfile = os.path.join(seq_func_folder, f\"{seq_id}_hmm_online.xml\")\n",
        "            with open(hmm_outfile, \"w\") as hfile:\n",
        "                hfile.write(hmm_xml)\n",
        "            print(f\"Online HMMER results saved to {hmm_outfile}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during online HMMER for {seq_id}: {e}\")\n",
        "            hmm_xml = \"\"\n",
        "    else:\n",
        "        hmm_outfile = os.path.join(seq_func_folder, f\"{seq_id}_hmm.tbl\")\n",
        "        cmd = f\"hmmscan --tblout {hmm_outfile} Pfam-A.hmm {seq_id}_prot.faa\"\n",
        "        try:\n",
        "            subprocess.run(cmd, shell=True, check=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error running local hmmscan for {seq_id}: {e}\")\n",
        "            hmm_outfile = \"\"\n",
        "        hmm_xml = \"\"\n",
        "\n",
        "    def parse_hmmer_xml(xml_content):\n",
        "        domains = []\n",
        "        try:\n",
        "            parser = etree.XMLParser(recover=True)\n",
        "            root = etree.fromstring(xml_content.encode(\"utf-8\"), parser=parser)\n",
        "            data = root.find(\".//data[@name='results']\")\n",
        "            if data is None:\n",
        "                return domains\n",
        "            for hit in data.findall(\"hits\"):\n",
        "                hit_name = hit.get(\"name\") or \"UnknownHit\"\n",
        "                for dom in hit.findall(\"domains\"):\n",
        "                    dom_name = dom.get(\"alihmmname\") or hit_name\n",
        "                    dom_evalue = dom.get(\"ievalue\") or dom.get(\"evalue\") or \"N/A\"\n",
        "                    domains.append(f\"{dom_name} (E={dom_evalue})\")\n",
        "        except Exception as e:\n",
        "            print(\"Error parsing HMMER XML:\", e)\n",
        "        return domains\n",
        "\n",
        "    domains = parse_hmmer_xml(hmm_xml) if hmm_xml else []\n",
        "    domain_pred = \"; \".join(domains) if domains else \"No domains detected\"\n",
        "    domain_predictions.append([seq_id, domain_pred])\n",
        "    organism_overall.append([seq_id, organism_pred])\n",
        "\n",
        "    for f in os.listdir(seq_func_folder):\n",
        "        shutil.copy(os.path.join(seq_func_folder, f), os.path.join(seq_org_folder, f))\n",
        "\n",
        "df_domains = pd.DataFrame(domain_predictions, columns=[\"Sequence_ID\", \"Domain_Predictions\"])\n",
        "domain_csv = os.path.join(dirs[\"functional_prediction\"], \"domain_predictions.csv\")\n",
        "df_domains.to_csv(domain_csv, index=False)\n",
        "print(f\"Domain predictions saved to: {domain_csv}\")\n",
        "\n",
        "df_org_func = pd.DataFrame(organism_predictions, columns=[\"Sequence_ID\", \"Predicted_Organism\", \"Function_Prediction\"])\n",
        "org_func_csv = os.path.join(dirs[\"functional_prediction\"], \"organism_predictions.csv\")\n",
        "df_org_func.to_csv(org_func_csv, index=False)\n",
        "print(f\"Organism predictions saved to: {org_func_csv}\")\n",
        "\n",
        "df_org_overall = pd.DataFrame(organism_overall, columns=[\"Sequence_ID\", \"Predicted_Organism\"])\n",
        "org_overall_csv = os.path.join(dirs[\"organism_origin\"], \"organism_prediction.csv\")\n",
        "df_org_overall.to_csv(org_overall_csv, index=False)\n",
        "print(f\"Overall organism predictions saved to: {org_overall_csv}\")\n",
        "\n",
        "print(\"\\n=== Functional & Organism Prediction Pipeline Completed ===\")"
      ],
      "metadata": {
        "id": "qd9h-QmVf3P_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}